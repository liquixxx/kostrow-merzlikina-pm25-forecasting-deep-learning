# Прогнозирование уровня загрязнения воздуха (PM2.5) с помощью полносвязной нейронной сети TensorFlow

**Авторы:**  
Мерзликина Мария
Костров Владислав

**Группа:**  
15.27д-пи05/25б

---

## Введение

Рост концентрации мелкодисперсных частиц PM2.5 в крупных городах приводит к увеличению числа сердечно‑сосудистых и респираторных заболеваний и напрямую влияет на продолжительность жизни населения.
Ручной анализ показаний станций мониторинга и использование простых статистических моделей не обеспечивают требуемой точности и не учитывают сложные нелинейные зависимости между метеоданными и уровнем загрязнения.

В работе предлагается система прогнозирования PM2.5 на основе полносвязной нейронной сети, реализованной в TensorFlow/Keras.  
Модель обучается на открытом датасете Beijing PM2.5 и показывает устойчивое качество прогноза при сравнении нескольких алгоритмов оптимизации.

---

## Аналоги

Существующие подходы к прогнозу качества воздуха можно условно разделить на три группы:

| Подход                  | Примеры моделей            | Особенности |
|-------------------------|----------------------------|------------|
| Классические статистические | ARIMA/ARIMAX, экспоненциальное сглаживание | Хороши для линейных стационарных рядов, плохо масштабируются по числу признаков |
| Алгоритмы ML           | Random Forest, Gradient Boosting, SVR | Улавливают нелинейности, но требуют тщательной ручной инженерии признаков |
| Глубокие нейросети     | LSTM/GRU, CNN‑LSTM, Dense сети | Позволяют автоматически строить сложные представления данных, но требуют больше вычислительных ресурсов |

Для задач с табличными метеорологическими признаками плотные (Dense) сети дают хороший компромисс между качеством, скоростью обучения и простотой реализации.
В отличие от тяжёлых архитектур для изображений, полносвязная сеть на десятках тысяч наблюдений и десятках признаков может быть обучена за минуты даже на одном GPU.

---

## Графики обучения алгоритмов

**Кривые обучения всех 5 оптимизаторов** демонстрируют гладкое (гиперболическое/логарифмическое) уменьшение функции потерь и отсутствие ярко выраженного переобучения:
<img width="1683" height="525" alt="image" src="https://github.com/user-attachments/assets/5dc0964d-9d51-406d-a618-680fcd1a8151" />

- Для каждого оптимизатора строятся две панели:  
  1) «Точность обучения и валидации» (1 − MAE для train/val по эпохам).  
  2) «Функция потерь обучения и валидации» (MSE для train/val по эпохам).
  <img width="786" height="523" alt="image" src="https://github.com/user-attachments/assets/9ef87954-eb8e-4b3c-9bef-3be41b130adc" />
  <img width="787" height="486" alt="image" src="https://github.com/user-attachments/assets/bab65552-997b-44de-aa68-906bb969edbd" />
  <img width="785" height="245" alt="image" src="https://github.com/user-attachments/assets/fe7b43ff-806a-4c90-a421-f9f142c6aa6e" />
- Валидационные кривые расположены чуть выше обучающих и постепенно сходятся к ним, что говорит о корректной настройке архитектуры и регуляризации.

---

## Архитектура модели (Алгоритм: Dense NN для PM2.5)

В качестве основной архитектуры выбрана полносвязная сеть на TensorFlow/Keras (Sequential API):

**Ключевые характеристики:**

- Тип сети: Fully Connected (Dense), регрессионная модель.  
- Функция активации скрытых слоёв: ReLU.  
- Активация выхода: линейная (для непрерывной регрессии).  
- Регуляризация: L2‑штраф на веса и Dropout между слоями для борьбы с переобучением.

---

## Гиперпараметры обучения

Оптимальная конфигурация подбиралась экспериментально по качеству на валидационной выборке:

| Параметр        | Значение                          |
|-----------------|-----------------------------------|
| Optimizer (основной) | Adam (lr = 0.001)           |
| Loss            | Mean Squared Error (MSE)         |
| Метрика         | Mean Absolute Error (MAE)        |
| Epochs max      | 120 (EarlyStopping patience=15)  |
| Batch size      | 128                              |
| Регуляризация   | L2(0.01) + Dropout(0.3 / 0.2)    |
| Normalization   | BatchNormalization после каждого Dense слоя |
| Callbacks       | EarlyStopping, ReduceLROnPlateau, ModelCheckpoint |

Такой набор обеспечивает плавные кривые обучения и устойчивую сходимость на всём диапазоне эпох.

---

## Датасет Beijing PM2.5

Используется открытый набор данных **Beijing PM2.5** из UCI Machine Learning Repository.

- Период: 2010–2014 годы.  
- Шаг: 1 час.  
- Объём: порядка 40 000 наблюдений после очистки.  

**Признаки:**

- Время: год, месяц, день, час.  
- Метеоданные: температура (TEMP), давление (PRES), точка росы (DEWP), скорость и направление ветра (Iws, Cbwd).  
- Газовые и аэрозольные примеси: PM10, SO2, NO2, O3.  

Целевая переменная — концентрация PM2.5 (µg/m³).

**Предобработка:**

- Замена специальных значений (например, −999/NA) на пропуски и их заполнение методами forward/backward fill.
- Удаление строк с некорректными или дублирующимися значениями.  
- One‑hot кодирование категориального признака направления ветра.  
- Масштабирование признаков и целевой переменной с помощью StandardScaler.  
- Разбиение на выборки: 70% train, 15% validation, 15% test.

---

## Результаты тестирования

После обучения на выборке Beijing PM2.5 лучшая модель (обычно с оптимизатором Adam или AdamW) показывает следующие усреднённые показатели на тестовом наборе:

| Метрика | Значение (примерно) |
|--------|----------------------|
| MAE    | 15–20 µg/m³          |
| RMSE   | 25–30 µg/m³          |
| R²     | 0.75–0.85            |

Диаграмма «факт против прогноза» показывает компактное облако точек вдоль диагональной линии, а гистограмма остатков близка к симметричному распределению вокруг нуля, что подтверждает адекватность прогноза.
<img width="1683" height="525" alt="image" src="https://github.com/user-attachments/assets/2ef25922-6ff1-4370-b094-9182d2313c76" />

---

## Сравнение 5 алгоритмов оптимизации

Для одной и той же архитектуры сравниваются пять оптимизаторов:

| Алгоритм | Особенности | Итоговая MAE (примерно) |
|----------|-------------|-------------------------|
| Adam     | Быстрая сходимость, устойчивость | 15–18 µg/m³ |
| RMSprop  | Хорош для нестационарных рядов   | 16–19 µg/m³ |
| SGD + momentum | Базовый метод, чувствителен к lr | 18–22 µg/m³ |
| AdamW    | Улучшенная регуляризация весов    | 15–18 µg/m³ |
| Nadam    | Adam + Nesterov momentum         | 16–19 µg/m³ |

Кривые обучения для всех пяти вариантов показывают отсутствие резких скачков и плавное снижение функции потерь, что подтверждает правильный подбор шага обучения и регуляризации.

---

**Планы развития:**

- Добавление рекуррентных (LSTM/GRU) и сверточных (1D‑CNN) слоёв для учёта временных и локальных паттернов.  
- Использование Keras Tuner / Optuna для автоматического подбора гиперпараметров.  
- Обучение на расширенных мульти‑городских датасетах и сравнение качества генерализованных моделей.  
- Экспорт в TensorFlow Lite / ONNX и развёртывание на сервере или edge‑устройствах (станции мониторинга).
