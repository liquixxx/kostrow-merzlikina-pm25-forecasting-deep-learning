# -*- coding: utf-8 -*-
"""FINALFINAL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dcMUj-dV1lWJArrNn69614rV__vqccNU
"""

# ============================================================
# –ü–û–õ–ù–û–°–í–Ø–ó–ù–ê–Ø –ù–° –î–õ–Ø –ü–†–û–ì–ù–û–ó–ê –ó–ê–ì–†–Ø–ó–ù–ï–ù–ò–Ø –í–û–ó–î–£–•–ê (PM2.5)
# –ü–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –∫–æ–¥ –¥–ª—è Google Colab
# ============================================================

!pip install ucimlrepo tensorflow plotly kaleido -q

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from ucimlrepo import fetch_ucirepo
import warnings, os, json
warnings.filterwarnings("ignore")

print("TensorFlow:", tf.__version__)
print("GPU devices:", tf.config.list_physical_devices("GPU"))

# ------------------------------------------------------------
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –û–°–ú–û–¢–† –î–ê–¢–ê–°–ï–¢–ê BEIJING PM2.5 (UCI, id=381)
# ------------------------------------------------------------
beijing_pm25 = fetch_ucirepo(id=381)  # Beijing PM2.5 dataset [web:10][web:56]

X_raw = beijing_pm25.data.features
y_raw = beijing_pm25.data.targets

df = pd.concat([X_raw, y_raw], axis=1)

print("\n–ö–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
print(df.columns.tolist())

# –í UCI —Ü–µ–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è 'pm2.5' (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä).[web:10]
target_col = "pm2.5"
assert target_col in df.columns, "–í df –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ 'pm2.5' ‚Äì –ø–æ—Å–º–æ—Ç—Ä–∏ —Å–ø–∏—Å–æ–∫ –≤—ã—à–µ –∏ –ø–æ–¥—Å—Ç–∞–≤—å –Ω—É–∂–Ω–æ–µ –∏–º—è."

# –≥—Ä—É–±–∞—è –æ—á–∏—Å—Ç–∫–∞: –∑–∞–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏, –∑–∞—Ç–µ–º forward/backward fill
df = df.replace(-999.0, np.nan)
df = df.replace("NA", np.nan)
df.fillna(method="ffill", inplace=True)
df.fillna(method="bfill", inplace=True)
df.dropna(inplace=True)

feature_cols = [c for c in df.columns if c != target_col]

print("\n–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:", df.shape)
print(df[[target_col]].describe())

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
fig = px.histogram(df, x=target_col, nbins=60,
                   title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ PM2.5 (Beijing)")
fig.show()

# ------------------------------------------------------------
# 2. –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í
# ------------------------------------------------------------
X = df[feature_cols].copy()
y = df[target_col].astype(float).values.reshape(-1, 1)

# one‚Äëhot –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'cbwd')[web:10]
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)

scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_scaled = scaler_X.fit_transform(X)
y_scaled = scaler_y.fit_transform(y).ravel()

X_train, X_temp, y_train, y_temp = train_test_split(
    X_scaled, y_scaled, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42
)

print("\nTrain:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

# ------------------------------------------------------------
# 3. –ü–ê–†–ê–ú–ï–¢–†–ò–ó–£–ï–ú–ê–Ø –ü–û–õ–ù–û–°–í–Ø–ó–ù–ê–Ø –ú–û–î–ï–õ–¨
# ------------------------------------------------------------
def create_dense_model(
    input_dim,
    n1=256,
    n2=128,
    n3=64,
    dropout1=0.3,
    dropout2=0.2,
    l2_reg=1e-2,
):
    reg = keras.regularizers.l2(l2_reg)
    model = keras.Sequential(
        [
            layers.Input(shape=(input_dim,)),
            layers.Dense(n1, activation="relu", kernel_regularizer=reg),
            layers.BatchNormalization(),
            layers.Dropout(dropout1),

            layers.Dense(n2, activation="relu", kernel_regularizer=reg),
            layers.BatchNormalization(),
            layers.Dropout(dropout2),

            layers.Dense(n3, activation="relu", kernel_regularizer=reg),
            layers.BatchNormalization(),
            layers.Dropout(dropout2),

            layers.Dense(1, activation="linear"),
        ]
    )
    return model

input_dim = X_train.shape[1]

# ------------------------------------------------------------
# 4. 5 –†–ê–ó–ù–´–• –ê–õ–ì–û–†–ò–¢–ú–û–í / –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–û–í (–¢–†–ï–ë–û–í–ê–ù–ò–ï –ö–£–†–°–ê)
# ------------------------------------------------------------
optimizers = {
    "Adam": keras.optimizers.Adam(learning_rate=1e-3),
    "RMSprop": keras.optimizers.RMSprop(learning_rate=1e-3),
    "SGD": keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9),
    "AdamW": keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=4e-3),
    "Nadam": keras.optimizers.Nadam(learning_rate=1e-3),
}

histories = {}
results = {}
models_paths = {}

os.makedirs("models_pm25", exist_ok=True)

for name, opt in optimizers.items():
    print(f"\n===== –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º {name} =====")

    model = create_dense_model(input_dim=input_dim)

    model.compile(optimizer=opt, loss="mse", metrics=["mae"])

    ckpt_path = f"models_pm25/best_{name.lower()}.keras"
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=15, restore_best_weights=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=7, verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            ckpt_path, monitor="val_loss", save_best_only=True
        ),
    ]

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=120,
        batch_size=128,
        verbose=0,
        callbacks=callbacks,
    )

    histories[name] = history.history
    models_paths[name] = ckpt_path

    # –û—Ü–µ–Ω–∫–∞
    test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
    y_pred_scaled = model.predict(X_test, verbose=0).ravel()

    # –æ–±—Ä–∞—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).ravel()
    y_pred_orig = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()

    mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)
    rmse_orig = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))
    r2 = r2_score(y_test_orig, y_pred_orig)

    results[name] = {
        "test_mae_scaled": float(test_mae),
        "test_rmse_scaled": float(np.sqrt(test_loss)),
        "test_mae_orig": float(mae_orig),
        "test_rmse_orig": float(rmse_orig),
        "r2_score": float(r2),
    }

    print(f"{name}: MAE = {mae_orig:.2f} ¬µg/m¬≥, RMSE = {rmse_orig:.2f}, R¬≤ = {r2:.3f}")

# –¢–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
metrics_df = pd.DataFrame(results).T.sort_values("test_mae_orig")
display(metrics_df)

# ------------------------------------------------------------
# 5. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ì–ò–ü–ï–†–ë–û–õ–ò–ß–ï–°–ö–ò–•/–õ–û–ì–ê–†–ò–§–ú–ò–ß–ï–°–ö–ò–• –ö–†–ò–í–´–•
# ------------------------------------------------------------
# –õ–æ–≥–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
fig_loss = make_subplots(rows=1, cols=2,
                         subplot_titles=["Train/Val Loss (log scale)", "MAE –ø–æ —ç–ø–æ—Ö–∞–º"],
                         specs=[[{"secondary_y": False}, {"secondary_y": False}]])

colors = px.colors.qualitative.Set1

for i, (name, hist) in enumerate(histories.items()):
    c = colors[i % len(colors)]

    # –ª–æ–≥–∞—Ä–∏—Ñ–º loss –¥–∞—ë—Ç –ø–ª–∞–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–µ/–ª–æ–≥-–∫—Ä–∏–≤—ã–µ
    train_log = np.log(hist["loss"])
    val_log = np.log(hist["val_loss"])

    fig_loss.add_trace(
        go.Scatter(
            x=np.arange(len(train_log)),
            y=train_log,
            name=f"{name} train",
            line=dict(color=c),
        ),
        row=1,
        col=1,
    )
    fig_loss.add_trace(
        go.Scatter(
            x=np.arange(len(val_log)),
            y=val_log,
            name=f"{name} val",
            line=dict(color=c, dash="dash"),
        ),
        row=1,
        col=1,
    )

    fig_loss.add_trace(
        go.Scatter(
            x=np.arange(len(hist["mae"])),
            y=hist["mae"],
            name=f"{name} mae",
            line=dict(color=c),
        ),
        row=1,
        col=2,
    )

fig_loss.update_xaxes(title_text="–≠–ø–æ—Ö–∏", row=1, col=1)
fig_loss.update_yaxes(title_text="log(MSE)", row=1, col=1)
fig_loss.update_xaxes(title_text="–≠–ø–æ—Ö–∏", row=1, col=2)
fig_loss.update_yaxes(title_text="MAE (scaled)", row=1, col=2)
fig_loss.update_layout(title="–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –∏ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–µ)")
fig_loss.show()
# ------------------------------------------------------------
# 5–ê. –û–¢–î–ï–õ–¨–ù–´–ï –ì–†–ê–§–ò–ö–ò –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ê–õ–ì–û–†–ò–¢–ú–ê
#    (—Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –ø–æ —ç–ø–æ—Ö–∞–º)
# ------------------------------------------------------------
for name, hist in histories.items():
    # —Å–æ–∑–¥–∞—ë–º figure —Å –¥–≤—É–º—è –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    fig, axes = plt.subplots(1, 2, figsize=(10, 3))
    fig.suptitle(f"–ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {name}", fontsize=12)

    # ---- ¬´–¢–æ—á–Ω–æ—Å—Ç—å¬ª –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ----
    # –±–µ—Ä—ë–º MAE –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –ø—Å–µ–≤–¥–æ‚Äë—Ç–æ—á–Ω–æ—Å—Ç—å: accuracy = 1 - mae
    train_mae = np.array(hist["mae"])
    val_mae   = np.array(hist["val_mae"])
    train_acc = 1.0 - train_mae
    val_acc   = 1.0 - val_mae

    axes[0].plot(train_acc, label="–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
    axes[0].plot(val_acc, label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å")
    axes[0].set_title("–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    axes[0].set_xlabel("–≠–ø–æ—Ö–∞")
    axes[0].set_ylabel("–¢–æ—á–Ω–æ—Å—Ç—å")
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # ---- –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ----
    axes[1].plot(hist["loss"], label="–û–±—É—á–∞—é—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
    axes[1].plot(hist["val_loss"], label="–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
    axes[1].set_title("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    axes[1].set_xlabel("–≠–ø–æ—Ö–∞")
    axes[1].set_ylabel("–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å")
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


# ------------------------------------------------------------
# 6. –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø, –°–•–û–î–ò–ú–û–°–¢–¨, –û–°–¢–ê–¢–ö–ò
# ------------------------------------------------------------
best_name = metrics_df.index[0]
print("\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ MAE:", best_name)

best_model = keras.models.load_model(models_paths[best_name])

y_pred_best_scaled = best_model.predict(X_test, verbose=0).ravel()
y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).ravel()
y_pred_best = scaler_y.inverse_transform(
    y_pred_best_scaled.reshape(-1, 1)
).ravel()

residuals = y_test_orig - y_pred_best

# scatter –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
fig_pred = make_subplots(
    rows=1,
    cols=2,
    subplot_titles=["–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ", "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤"],
)

fig_pred.add_trace(
    go.Scatter(
        x=y_test_orig,
        y=y_pred_best,
        mode="markers",
        name="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è",
        marker=dict(color="royalblue", opacity=0.5),
    ),
    row=1,
    col=1,
)
fig_pred.add_trace(
    go.Scatter(
        x=y_test_orig,
        y=y_test_orig,
        mode="lines",
        name="–ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è",
        line=dict(color="black", dash="dash"),
    ),
    row=1,
    col=1,
)

fig_pred.add_trace(
    go.Histogram(
        x=residuals,
        nbinsx=60,
        name="Residuals",
        marker=dict(color="indianred"),
    ),
    row=1,
    col=2,
)

fig_pred.update_xaxes(title_text="–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π PM2.5 (¬µg/m¬≥)", row=1, col=1)
fig_pred.update_yaxes(title_text="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π PM2.5 (¬µg/m¬≥)", row=1, col=1)
fig_pred.update_xaxes(title_text="–û—Å—Ç–∞—Ç–æ–∫ (—Ñ–∞–∫—Ç ‚àí –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)", row=1, col=2)
fig_pred.update_layout(title="–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏ –∞–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤")
fig_pred.show()

# ------------------------------------------------------------
# 7. –°–û–ó–î–ê–ù–ò–ï –ü–†–û–°–¢–û–ì–û HTML‚Äë–ü–†–ò–õ–û–ñ–ï–ù–ò–Ø –° –†–ï–ó–£–õ–¨–¢–ê–¢–ê–ú–ò
# ------------------------------------------------------------
summary = {
    "best_model": best_name,
    "metrics": results[best_name],
    "all_metrics": results,
}

with open("pm25_summary.json", "w") as f:
    json.dump(summary, f, indent=2)

html = f"""
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>PM2.5 Forecasting ‚Äì TensorFlow Dense NN</title>
  <style>
    body {{
      font-family: Arial, sans-serif;
      background: linear-gradient(135deg,#243B55,#141E30);
      color: #f5f5f5;
      padding: 30px;
    }}
    .card {{
      background: rgba(255,255,255,0.08);
      border-radius: 16px;
      padding: 20px 25px;
      margin-bottom: 20px;
    }}
    h1 {{ color: #FFD700; }}
    table {{
      border-collapse: collapse;
      width: 100%;
      margin-top: 10px;
    }}
    th, td {{
      border: 1px solid rgba(255,255,255,0.2);
      padding: 6px 10px;
      text-align: center;
    }}
    th {{ background: rgba(255,255,255,0.15); }}
  </style>
</head>
<body>
  <div class="card">
    <h1>üè≠ –ü—Ä–æ–≥–Ω–æ–∑ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –≤–æ–∑–¥—É—Ö–∞ (PM2.5)</h1>
    <p>–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å TensorFlow –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö Beijing PM2.5 (UCI). [web:10]</p>
  </div>

  <div class="card">
    <h2>–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å</h2>
    <p><b>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä:</b> {best_name}</p>
    <p><b>MAE:</b> {results[best_name]['test_mae_orig']:.2f} ¬µg/m¬≥</p>
    <p><b>RMSE:</b> {results[best_name]['test_rmse_orig']:.2f} ¬µg/m¬≥</p>
    <p><b>R¬≤:</b> {results[best_name]['r2_score']:.3f}</p>
  </div>

  <div class="card">
    <h2>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤</h2>
    <table>
      <tr><th>–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä</th><th>MAE (¬µg/m¬≥)</th><th>RMSE (¬µg/m¬≥)</th><th>R¬≤</th></tr>
      {''.join(f"<tr><td>{name}</td><td>{m['test_mae_orig']:.2f}</td><td>{m['test_rmse_orig']:.2f}</td><td>{m['r2_score']:.3f}</td></tr>" for name,m in results.items())}
    </table>
  </div>

  <div class="card">
    <h2>–ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏</h2>
    <ul>
      <li>–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏–≤—ã–µ loss –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–ª–∞–≤–Ω—ã–º–∏ –∏ —Å—Ö–æ–¥–∏—Ç—å—Å—è –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É (train –∏ val), —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ–± —É—Å—Ç–æ–π—á–∏–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏.</li>
      <li>–û–±–ª–∞–∫–æ ¬´—Ñ–∞–∫—Ç vs –ø—Ä–æ–≥–Ω–æ–∑¬ª –¥–æ–ª–∂–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞—Ç—å—Å—è –∫ –¥–∏–∞–≥–æ–Ω–∞–ª–∏, –∞ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ ‚Äî –±—ã—Ç—å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π –≤–æ–∫—Ä—É–≥ –Ω—É–ª—è.</li>
    </ul>
  </div>
</body>
</html>
"""

with open("pm25_app.html", "w", encoding="utf-8") as f:
    f.write(html)

print("\nHTML‚Äë–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: pm25_app.html")
print("–ö—Ä–∞—Ç–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:", summary["metrics"])

# ------------------------------------------------------------
# 8. –°–û–•–†–ê–ù–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –î–õ–Ø –î–ê–õ–¨–ù–ï–ô–®–ï–ì–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# ------------------------------------------------------------
best_model.save("best_pm25_model.keras")
print("–§–∞–π–ª –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: best_pm25_model.keras")
